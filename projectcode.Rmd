---
title: "Group-L-Project"
author: "Laknath Dias Gunathilake, Alexander Talbott, and Meghna Subramanian"
date: "March 13, 2021"
output: html_document
---
This is the group project for Group L in 95-791 Data Mining at Carnegie Mellon University, Heinz College, Spring 2021. The project entails an analysis of the network infrastructure of XYZ Bank in St. Loius, Missouri. The team at the bank recently discovered many instances of network intrusions and labeled them in a data set with other information. This project attempts to build a data mining approach to identifying such network intrusions in the future.

In this file, you will find the approach we took to cleaning the data, building classification models to detect likely intrusions, and building clustering models to categorize the types of intrusions and identify new intrusions in the future. 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)

library(rpart) # added tree library
library(partykit)
library(ggplot2)
library(gridExtra)
library(dplyr)
library(ggcorrplot)
library(randomForest)
library(cluster)
library(caret)
library(leaps)
library(Rtsne)

df <- read.csv("network_traffic.csv")

cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

set.seed(1234)
```

## Data Cleaning
Unfortunately, the data provided by the bank is not perfect. Here, we drop the columns that will not be useful in our analysis as they contain the same value for every instance and we convert the data to datatypes R can better understand.
```{r}
# drop useless columns
df <- select(df, -c(num_outbound_cmds, land, num_failed_logins, wrong_fragment, urgent, is_host_login, su_attempted))

# Clean is_intrusion column
df$is_intrusion <- ifelse(df$is_intrusion == 1, 1, 0)

# make Yes/No variables more readable
df <- df %>%
  mutate(is_intrusion = ifelse(is_intrusion == 0,"No","Yes")) %>%
  mutate(logged_in = ifelse(logged_in == 0,"No","Yes")) %>%
  mutate(root_shell = ifelse(root_shell == 0,"No","Yes")) %>%
  mutate(is_guest_login = ifelse(is_guest_login == 0,"No","Yes"))

# convert to factors
df$is_intrusion <- as.factor(df$is_intrusion)
df$protocol_type <- as.factor(df$protocol_type)
df$service <- as.factor(df$service)
df$flag <- as.factor(df$flag)
df$logged_in <- as.factor(df$logged_in)
df$root_shell <- as.factor(df$root_shell)
df$is_guest_login <- as.factor(df$is_guest_login)
```

## Data Exploration  

In this section, we explore the various connection types and services to determine if we are able to detect any interesting patterns or anomolies with regards to bytes been transferred and the duration of the connections. 

```{r }
p1 <- qplot(df$duration, geom="histogram", xlab="Duration")
p2 <- qplot(df$protocol_type, geom="bar", xlab="Protocol Type")
p3 <- qplot(df$service, geom="bar", xlab="Service")
p4 <- qplot(df$src_bytes, geom="histogram", xlab="Bytes from Source to Destination")
p5 <- qplot(df$dst_bytes, geom="histogram", xlab="Bytes from Destination to Source")
p6 <- qplot(df$flag, geom="bar", xlab="Error Status")
p7 <- qplot(df$hot, geom="histogram", xlab="Hot Indicators")
p8 <- qplot(df$logged_in, geom="bar", xlab="Logged In")
p9 <- qplot(df$num_compromised, geom="histogram", xlab="Number Compromised")
p10 <- qplot(df$root_shell, geom="bar", xlab="Root Shell")
p11 <- qplot(df$num_root, geom="histogram", xlab="Number of Root Commands")
p12 <- qplot(df$num_file_creations, geom="histogram", xlab="File Creations")
p13 <- qplot(df$num_shells, geom="histogram", xlab="Number of Shells")
p14 <- qplot(df$num_access_files, geom="histogram", xlab="Accessed Files")
p15 <- qplot(df$is_guest_login, geom="bar", xlab="Guest Login")
p16 <- qplot(df$is_intrusion, geom="bar", xlab="Intrusion")
grid.arrange(p1, p2, p3, p4, p5, p6, p7, p8, p9, ncol=3)
grid.arrange(p10, p12, p13, p14, p15, p16, ncol=3)

#Adding a boxplots src and dst bytes by service
options(scipen=999)
ggplot(data = df,aes(x = as.factor(service),y = src_bytes))+
        geom_boxplot()+
        coord_flip()+ylab("number of data bytes from source to destination ")+xlab("Service")

ggplot(data = df,aes(x = as.factor(service),y = dst_bytes))+
          geom_boxplot()+
          coord_flip()+
          ylab("number of data bytes from destination to source")+xlab("Service")
```
## Univariate Analysis

+ Looking at the data above it is evident that a majority of the continous variables are highly skewed with most values centering close to zero and and a few large values that skewing the distributions to the right.

+ it also evident that there are a number of variables that contain only zero's 

+ accordingly we hope to look at a bivariate analysis between two variables of interest in the next section


## Analyzing byte transfers for Different Service Types

We wanted to understand the bytes transferred between the different services `r unique(df$service)`


+  It is evident that the http ftp_data,http and smtp data have large variation in the number of bytes sent from source to destination with http data having an extreme outlier

+ When looking at byte transfer from destination to the source it is evident that http, and telnet have large variations.
It is evident that "Telnet is not a secure protocol and is unencrypted. By monitoring a user's connection, anyone can access a person's username, password and other private information that is typed over the Telnet session in plaintext. With this information, access can be gained to the user's device." Source("https://searchnetworking.techtarget.com/definition/Telnet")
According this is quite alarming that we large number of bytes being sent from destination to the source using the telnet service type.

# Understanding Duration for different Service Types

```{r}
ggplot(data = df,aes(x = as.factor(service),y = duration ))+
          geom_boxplot()+
          coord_flip()+xlab("Service Types")
```

+ It is evident telnet and service type tagged others have the highest duration or the number of second in the connection. 
It is also evident that others seem to have multiple outliers including an outlier that incidates there was a connection that 
lasted for more than `r max(df$duration)/(60*60)` or 6 hours


# Class Metrics Function
```{r}
classMetrics <- function(score, y, cutoff, 
                         type = c("all", "accuracy", "sensitivity", 
                                  "specificity", "ppv", "npv", "precision", 
                                  "recall")) {
  
              
  # classify the observations based on score and cutoff
  observed <- y
  predicted <- ifelse(score >= cutoff,1,0)
  
      # Form confusion matrix
  conf<-table(predicted,observed)
  
  # Use the appropriate entries in the confusion matrix to calculate all metrics:
  # accuracy, sensitivity, specificity, ppv, npv, precision (which is the same as ppv), recall (which is the same as sensitivity)
  #accuracy <-
  
  accuracy<- round((conf[4]+conf[1])/sum(conf),6)
  sensitivity <- (conf[4])/(conf[3]+conf[4])
  specificity <-  conf[1]/(conf[1]+conf[2])
  ppv         <- conf[4]/(conf[4]+conf[2])
  npv         <- conf[1]/(conf[1]+conf[3])
  precision   <- ppv
  recall      <- sensitivity
  
  # I'm giving you the exact names you should use in your output dataframe  
  metric.names <- c("accuracy", "sensitivity", "specificity", 
                    "ppv", "npv", "precision", "recall")

  # Form into data frame
  value <- c(accuracy, sensitivity, specificity, 
                    ppv, npv, precision, recall)
  
  
  
  # Your data frame should contain ONLY ONE COLUMN named "value"
  # assign the metric.names above as the rownames of your data frame (just as the rownames, not as a second column!)
  df <- data.frame(row.names = metric.names,value)
  
  full_list<- list(conf,df)
  names(full_list) <- c("conf.mat", "perf")
  #full_list <<- full_list
  # Return a list containing the confusion matrix and the data frame of the metrics
  # If "all", return all metrics. Otherwise, return just the requested subset of metrics
  if ("all" %in% type){
    return(full_list)
  } else {
    full_list[[2]]<- full_list[[2]]%>%
          filter(row.names(full_list[[2]]) %in% type)
    return(full_list)
  }
}

```


#Plot Class 
```{r}
plotClassMetrics <- function(score, y, xvar = NULL, yvar = c("accuracy", "sensitivity", 
                                  "specificity", "ppv", "npv", "precision", 
                                  "recall"),
                             flip.x = FALSE) {
  
  # matches the user-input yvar argument against the list of valid options specified in the function header
  yvar <- match.arg(yvar)
  
  # If there are a lot of (more than 100) unique score values, just calculate the metrics
  # along an evenly spaced grid of 100 scores.
  unique.scores <- unique(score)
  if(length(unique.scores) > 100) {
    cutoff.seq <- sample(unique.scores, 100, replace = FALSE)
  } else { # otherwise just use the unique score values to calculate the metrics
    cutoff.seq <- unique.scores
  }
  
  # number of score values to evaluate the metrics
  n <- length(cutoff.seq)
  
  # initialize the vectors to record the x & y coordinates
  x.out <- numeric(n)
  y.out <- numeric(n)
  # Loop over all values of the score and calculate the performance metrics
  for(i in 1:n) {
    if(!is.null(xvar)) { # specified metric as x axis values
      # call classMetrics to calculate the metrics and record the values
      metrics <- classMetrics(score, y, cutoff = cutoff.seq[i], type = c(xvar, yvar))
      x.out[i] <- metrics$perf[xvar, 1]
      y.out[i] <- metrics$perf[yvar, 1]
    } else { # score as x axis values
      # call classMetrics to calculate the metrics and record the values
      metrics <- classMetrics(score, y, cutoff = cutoff.seq[i], type = c(yvar))
      x.out[i] <- cutoff.seq[i]
      y.out[i] <- metrics$perf[yvar, 1]
    }
  }

  # flip x values if required
  if(flip.x) {
    x.out <- 1 - x.out
  }
  
  # Combine metrics into a data frame
  df.out <- data.frame(score = cutoff.seq, x = x.out, y = y.out)
  
  # Reorder the data frame in increasing order of the x-axis variable
  df.out <- df.out[order(df.out$score), ]
  
  # De-duplicate x-axis
  df.out <- subset(df.out, subset = !duplicated(df.out$x))
  
  # determine the x-axis label
  if(!is.null(xvar)) {
    x.text <- ifelse(flip.x, paste0("1 - ", xvar), xvar)
  } else {
    x.text <- "score"
  }
  
  # Construct line plot
  print(qplot(data = df.out, x = x, y = y, geom = "line",
              xlab = ifelse(is.null(xvar), "score", x.text),
              ylab = yvar, ylim = c(0, 1)))
}
```


# Identifying the imbalances in dataset
```{r}
ggplot(data = df,aes(x = is_intrusion))+
      geom_bar(aes(fill=is_intrusion))+ ggtitle("Class Imbalance")

```

##Oversampling (Upsampling) the minority class
+ It is evident from the above graph that we have a problem of imbalanced classes- Meaning we have far fewer postive classes
or actual intrusions compared to benign intrusions.

+ Since we cannot collect aditional data in this instance, such a situattion requires us to balance the classes by introducing
resampling to the dataset. 

+ Since the minority class is underpresented, we decided to oversample the `r is_intrusion=="Yes"` observations

```{r}
df<- df%>%select(-c(7:14))%>%select(-c(1,7))


# Upsample the data to artifically overcome sample imbalance
df.more.idx <- sample(which(df$is_intrusion == "Yes"), 900, replace = TRUE)

set.seed(981)


df.upsample <- rbind(df,
                        df[df.more.idx, ])


ggplot(data = df.upsample,aes(x = is_intrusion))+
      geom_bar(aes(fill=is_intrusion))+ ggtitle("Classes Balanced")

```
+ The above bar graph seems to indicate now that we have an approximate balance between intrusions equal to one and zero

```{r}
# Randomly select 20% of the data to be held out for model validation
test.indexes <- sample(1:nrow(df.upsample), 
                       round(0.2 * nrow(df.upsample)))


train.indexes <- setdiff(1:nrow(df.upsample), test.indexes)



df.train <- df.upsample[train.indexes,]
df.test <- df.upsample[test.indexes,]


```

## Decision Trees
### Training the tree
```{r}
intrusion.tree <- rpart(is_intrusion ~ ., df.train)


plot(intrusion.tree)
text(intrusion.tree)


intrusion.party <- as.party(intrusion.tree)

plot(intrusion.party)

print(intrusion.party)

```


### Growing and Pruning the Tree
```{r}
intrusion.full <- rpart(is_intrusion ~ ., data = df.train, 
                        control = rpart.control(minsplit=10, cp=0.002))

# Run the `plotcp` command on this tree. Also look at the `cptable` attribute of `marketing.full`
plotcp(intrusion.full)

intrusion.full$cptable

mycp <- 0.046

intrusion.pruned<- prune(intrusion.full,cp= mycp)


print(intrusion.pruned)
```
+ After trying different min split levels for training the dataset I found minsplit level of 10 to result in better performance when training the tree.


### Pruned Tree
```{r}
df.pruned.party <- as.party(intrusion.pruned)
plot(df.pruned.party, gp = gpar(fontsize = 7))
```

+ it is evident that in the pruned tree we split by service and flag to classify observations as intrusions or not.  

### Using the pruned tree on the test dataset
```{r}
my_pred<- predict(intrusion.pruned, df.test, type="prob")[,1]


classMetrics(my_pred, df.test$is_intrusion,cutoff = 0.20)
```

## Visualizing Sensitivity and Specificity
```{r}
plotClassMetrics(my_pred, df.test$is_intrusion, 
                 xvar = "accuracy", yvar = "specificity", 
                 flip.x = TRUE)
```
+ it is evident that our tree model has a sensitivity of 0.66. Sensitivity or recall is when given the observations that had the event (intrusions), the observations that were correctly classified to have the event.

+ We also know that a classifier with high Sensitivity is desirable when False Negatives (e.g., failing to detect an intrusion ) are more costly than False Positives(flagging a case an attemt that turns out to non intrusive)


## Building a Random forest 

Since Decision trees provided a high degree of recall, we also wanted look at random forests

### Building a Random forest 
```{r}
intrusion.rf <- randomForest(is_intrusion ~.,data=df.train)
intrusion.rf
```

+ Based on the importance plot, we know that service, flag, src_bytes, dst_bytes and protocol type are important 

```{r}
varImpPlot (intrusion.rf)
```

```{r}
rf.test.prob <- predict(intrusion.rf, newdata = df.test, type = "prob")[,1]

classMetrics(rf.test.prob, df.test$is_intrusion, cutoff = 0.3)
```


## Logistic Regression
In this section, we will attempt to separate intrusions from benign sessions using logistic regression. First we will select the subset of variables that will be best to fit the logisitc regression model.
```{r}
df.subset <- regsubsets(is_intrusion ~ .,
                        data = df,
                        nbest = 1,
                        nvmax = NULL,
                        method = "exhaustive", 
                        really.big = TRUE)

nvmax = length(df) # all variables
df.subset_bestmodels <- list()
#Iterate to get the best model at each number of coeficients
#Remember that we limited our nvmax to 5!
for(i in 1:nvmax) {
  df.subset_bestmodels[[i]] <- c(coef(df.subset, i))
}
df.subset_bestmodels
```
This shows us which variables are included at each step of the best subset selection. We start with a model consisting of one variable then work up to a full model with all 16 predictos included. We will now track the metrics of these models to pick which one is ideal for regression.

```{r}
df.summary<-summary(df.subset)

num_variables<-seq(1,length(df.summary$rss))

plot_RSS<-ggplot(data = data.frame(df.summary$rss),
                 aes(x=num_variables,y=df.summary$rss))+
  geom_line()+
  geom_point(x=which.min(df.summary$rss),
             y=min(df.summary$rss),aes(color="red"),
             show.legend = FALSE)+
  xlab("# Variables")+
  ylab("RSS")+
  theme_bw()

plot_R_sq<-ggplot(data = data.frame(df.summary$rsq),
                 aes(x=num_variables,y=df.summary.rsq))+
  geom_line()+
  geom_point(x=which.max(df.summary$rsq),
             y=max(df.summary$rsq),aes(color="red"),
             show.legend = FALSE)+
  xlab("# Variables")+
  ylab("R-sq")+
  theme_bw()

plot_BIC<-ggplot(data = data.frame(df.summary$bic),
                 aes(x=num_variables,y=df.summary.bic))+
  geom_line()+
  geom_point(x=which.min(df.summary$bic),
             y=min(df.summary$bic),aes(color="red"),
             show.legend = FALSE)+
  xlab("# Variables")+
  ylab("BIC")+
  theme_bw()

plot_AIC<-ggplot(data = data.frame(df.summary$cp),
                 aes(x=num_variables,y=df.summary.cp))+
  geom_line()+
  geom_point(x=which.min(df.summary$cp),
             y=min(df.summary$cp),aes(color="red"),
             show.legend = FALSE)+
  xlab("# Variables")+
  ylab("AIC")+
  theme_bw()


grid.arrange(plot_RSS, plot_R_sq,plot_AIC,plot_BIC, ncol=2,nrow=2)
```
Here we see the RSS, R2, AIC, and BIC of all of the above models. We can see that errors are minimized and R2 is maximized at about 6 variables included with severely diminishing returns after that amount. Therefore, we will select the subset with 6 variables to put into the logistic regression model.

```{r}
# fit logistic regression on train set
glm.fits <- glm(is_intrusionâˆ¼., data=df.train ,family="binomial", maxit=100)
summary(glm.fits)

# predict on test set
glm.probs <- predict(glm.fits,df.test, type="response")

cutoff <- 0.5
glm.pred <- 1:length(glm.probs)
glm.pred[glm.probs > cutoff] <- "Yes"
glm.pred[glm.probs < cutoff] <- "No"

table(glm.pred, df.test$is_intrusion)
varImp(glm.fits)
```

## Clustering
```{r}
# source: https://towardsdatascience.com/clustering-datasets-having-both-numerical-and-categorical-variables-ed91cdca0677

# Clustering
intrusion_df <- filter(df, is_intrusion == "Yes")

# build a df of gower distances
gower_df <- daisy(intrusion_df, metric="gower", type=list(logration=2))
summary(gower_df)

# calculate silhouette width to determine number of clusters
silhouette <- c()
silhouette = c(silhouette, NA)

# calculate distances using medoids instead of centroids
# this takes a long time to run
for(i in 2:10){
  pam_clusters = pam(as.matrix(gower_df),
                 diss = TRUE,
                 k = i)
  silhouette = c(silhouette,pam_clusters$silinfo$avg.width)
}

# plot silhouette widths based on number of clusters 
plot(1:10, silhouette,
     xlab = "Clusters",
     ylab = "Silhouette Width")
lines(1:10, silhouette)

pam <- pam(gower_df, diss = TRUE, k = 6)
# These are the median values of our clusters
# They represent the characteristics of each cluster
intrusion_df[pam$medoids, ]

# plot the data in 2-dimensional space
tsne_object <- Rtsne(gower_df, is_distance = TRUE)
tsne_df <- tsne_object$Y %>%
  data.frame() %>%
  setNames(c("X", "Y")) %>%
  mutate(cluster = factor(pam$clustering))

ggplot(aes(x = X, y = Y), data = tsne_df) +
  geom_point(aes(color = cluster))
```